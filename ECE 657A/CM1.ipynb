{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CM1-KarthikEdit",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmJh68-80ZiH",
        "outputId": "ccbab899-1ad4-434f-bbca-425b64328aa8"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install gensim\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj0dc6Dc0loB"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VCGxe5O0xZU"
      },
      "source": [
        "!git clone https://github.com/huggingface/datasets.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw2xrTwC1mzG",
        "outputId": "8a5f5b14-b20b-4eb1-c29b-56642eb6449b"
      },
      "source": [
        "!python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/0fd9e01360d229a22adfe0ab7e2dd2adc6e2b3d6d3db03636a51235947d4c6e9)\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IHeFUaK1smM",
        "outputId": "2d20f8f8-0c98-489e-dec2-fd00bd2f7cfb"
      },
      "source": [
        "!python -c \"from datasets import load_dataset; dataset = load_dataset('climate_fever')\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset climate_fever (/root/.cache/huggingface/datasets/climate_fever/default/1.0.1/3b846b20d7a37bc0019b0f0dcbde5bf2d0f94f6874f7e4c398c579f332c4262c)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLpGm_bS2owS"
      },
      "source": [
        "**Loading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrlaB3m-1vWl",
        "outputId": "42b6cbc8-d575-412a-e3da-881017c8f124"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('climate_fever')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset climate_fever (/root/.cache/huggingface/datasets/climate_fever/default/1.0.1/3b846b20d7a37bc0019b0f0dcbde5bf2d0f94f6874f7e4c398c579f332c4262c)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWHdKC0p2uFJ"
      },
      "source": [
        "**Building Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGgzwlTA2T2u"
      },
      "source": [
        "import re\n",
        "dataset_features = dataset['test']\n",
        "claim_evidences = []\n",
        "\n",
        "# for x in dataset_features:\n",
        "#     sclaim_evidences.append(x['claim'])\n",
        "#     dataset_evidences = dataset_features['evidences']\n",
        "#     for y in dataset_evidences: \n",
        "#       for z in y:        \n",
        "#         claim_evidences.append(z['evidence'])\n",
        "# print(claim_evidences)\n",
        "\n",
        "claims = []\n",
        "for x in dataset_features:\n",
        "  x_claim = re.sub(\"[^a-zA-Z]\", \" \",str(x['claim']))\n",
        "  #remove extra characters\n",
        "  x_claim = re.sub(r\"[[0-9]*\\]\", \" \", x_claim)\n",
        "  #remove the extra spaces between words\n",
        "  x_claim = re.sub(r\"\\s+\", \" \", x_claim)\n",
        "  #convert all letters to lowercase\n",
        "  x_claim = x_claim.lower()\n",
        "  claims.append(x_claim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtgjDSgG_6A"
      },
      "source": [
        "**Preprocessing: removing stopwords and tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCPgEK5vEx_n",
        "outputId": "a7e190d6-1838-42b8-c7cd-db2aabc6b329"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#tokenize list of sentences to list of words\n",
        "corpus = [nltk.word_tokenize(claim) for claim in claims]\n",
        "#define the english stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "#remove the stop words from the test\n",
        "for i, _ in enumerate(corpus):\n",
        "  corpus[i] = [word for word in corpus[i] if word not in stop_words]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjcVlQpQK1kx"
      },
      "source": [
        "**Embedding the entire dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIN5IkzTK4xR"
      },
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYANMOodKi9v"
      },
      "source": [
        "**Splitting the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvEzfTHSKYWI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "corpus_train, corpus_test = train_test_split(corpus, test_size = 0.2, random_state = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXM9jo81Vzk9"
      },
      "source": [
        "**CM 1 - Part 1 -- Word2Vec embedding on entire corpus and train**\n",
        "\n",
        "Why the Word2Vec parameters:\n",
        "\n",
        "Generally we can't determine the hyperparameters of a word2vec model since it is automatically learnt by the model in training.\n",
        "\n",
        "What we have done below is select and alter some parameters with some range of flexibility.\n",
        "\n",
        "For this model, we used the default model type of **CBOW** because we wanted the surrounding words to predict the word they flank and because our dataset of words is relatively small.\n",
        "\n",
        "We also picked a **minimum count of 1** because of the small dataset, we wanted as much of the words to be included.\n",
        "\n",
        "We selected the **window of words to be 7** because a cursory look of the sentences in the data showed that each sentence averaged 7 words.\n",
        "\n",
        "We picked a **dimensionality of 50 words** mostly because tests with a dimensionality of 100 and above didn't show significant differences in the vector models of the words possibly due to the size of the dataset and its general theme.\n",
        "\n",
        "Increasing the number of epochs  benefits the quality of the word representations. Therefore, we set the iter = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ShJzdISKvWs"
      },
      "source": [
        "#wv_model = Word2Vec(corpus, min_count=1, window=5, size=300, sample=0, sg=0, alpha=0.03, min_alpha=0.0007, negative=5) \n",
        "wv_model = Word2Vec(corpus, min_count=1, window=7, size=50, iter=100, workers = 20) \n",
        "#wv_model_train = Word2Vec(corpus, min_count=1, size=300, alpha=0.03, min_alpha=0.0007, negative=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3U4vbBTSIdh"
      },
      "source": [
        "Increasing the number of epochs benefits the quality of the word representations. Therefore, we set the iter = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17xBXSBIWmA1"
      },
      "source": [
        "**CM 1 - Part 2 -- Comparing the cosine similarity of a couple words**\n",
        "\n",
        "From the brief comparisons below based on the context of the document, we can see that the words that in context share a lot of similarity and could be used around each other typically have a high cosine similarity.\n",
        "\n",
        "The values below are arranged in order of decreasing similarity.\n",
        "\n",
        "For example: \n",
        "1. polar and bears (0.81767243) -> have an expected similarity because both words are typically found in the same context.\n",
        "2. climate and temparature (-0.25) -> This 02 words were probably not close to each other within sentences. Even though they have contextual similarity to some extent, the similarity score is low because of the dataset and training limitations.\n",
        "---------------------------------------------------------------\n",
        "Below, we take a look at the semantic relationship between a word and its most similar words based off this dataset::\n",
        "\n",
        "- For 'Man': 'made', 'contributions', 'legislators','believe', 'impacting'\n",
        "- For 'human': 'activities', 'primarily', 'largely', 'anthropogenic', 'civilization'\n",
        "- For 'warming': conspiring, adapting, dimming, global\n",
        "\n",
        "Considering the general context of this dataset, one can see why these words are the most similar to the selected words - they actually make contextual sense as the dataset details how **human** **activities** / **contributions** across the years have **caused** / effected a trend of climate **changes** / **warming** (**temperature**) of the planet.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0LcJerGWmPW",
        "outputId": "bc49909e-274f-47e0-9631-b057e531818b"
      },
      "source": [
        "#1st set of words: climate and warming\n",
        "print(wv_model.wv.similarity('climate', 'warning'))\n",
        "\n",
        "#2nd set of words: polar and bears\n",
        "print(wv_model.wv.similarity('polar', 'bears'))\n",
        "\n",
        "#3rd set of words: climate and temperature\n",
        "print(wv_model.wv.similarity('climate', 'temperature'))\n",
        "\n",
        "#4th set of words: hot and ice\n",
        "print(wv_model.wv.similarity('hot', 'ice'))\n",
        "\n",
        "#5th set of words: temperature and ice\n",
        "print(wv_model.wv.similarity('temperature', 'ice'))\n",
        "\n",
        "#6th set of words: earthquakes and extinction\n",
        "print(wv_model.wv.similarity('earthquakes', 'extinction'))\n",
        "\n",
        "#7th set of words: windmill and roman\n",
        "print(wv_model.wv.similarity('windmill', 'roman'))\n",
        "\n",
        "#8th set of words: residency and atmospheric\n",
        "print(wv_model.wv.similarity('residency', 'atmospheric'))\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "#List of the most similar words to random words in data set\n",
        "\n",
        "#1st word : man\n",
        "print(wv_model.wv.most_similar('man'))\n",
        "#2nd word: human\n",
        "print(wv_model.wv.most_similar('human'))\n",
        "#3rd word: warming\n",
        "print(wv_model.wv.most_similar('warming'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4332425\n",
            "0.81767243\n",
            "0.1554002\n",
            "-0.22689188\n",
            "0.07111336\n",
            "0.3665973\n",
            "0.41370222\n",
            "0.59930485\n",
            "[('made', 0.9193103909492493), ('contributions', 0.7849156856536865), ('legislators', 0.7373270392417908), ('prosperity', 0.730117678642273), ('illegal', 0.7162542939186096), ('aware', 0.7099815011024475), ('minimal', 0.708002507686615), ('believe', 0.701858401298523), ('courts', 0.6997411251068115), ('highlight', 0.691326916217804)]\n",
            "[('activities', 0.7813305854797363), ('primarily', 0.7728374600410461), ('largely', 0.7700099349021912), ('anthropogenic', 0.7380788922309875), ('aerosol', 0.7208808064460754), ('mainly', 0.7004191875457764), ('dominant', 0.6888383626937866), ('emissions', 0.6688092947006226), ('fingerprints', 0.6635338068008423), ('discredits', 0.6600527167320251)]\n",
            "[('dimming', 0.5790356397628784), ('cooling', 0.5767964124679565), ('global', 0.5641350746154785), ('post', 0.5369208455085754), ('adapting', 0.5351721048355103), ('sulfate', 0.5264771580696106), ('observed', 0.5218417048454285), ('regarding', 0.5205752849578857), ('anthropogenic', 0.5171043276786804), ('therefore', 0.5114220976829529)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKBdM5DM0KMd"
      },
      "source": [
        "**CM 1 - Part 3 -- Analyze the quality of embeddings**\n",
        "\n",
        "All of the arithmetic computations below except for the 1st and 3rd computation reveal relatively significant relationship between vectors on either side of the equation implying some correlation between semantic deduction and the vector embeddings. \n",
        "\n",
        "The 2nd computation \"carbon + oxygen =  co\" shows signficant relationship. This could be due to the high occurence of the words in the dataset and also the simplicity of the words.\n",
        "\n",
        "The 5th computation \"ice + melts =  sea + rises \" shows signficant relationship. Once again, this could be due to the high occurence of the words in the dataset and also the simplicity of the words.\n",
        "\n",
        "The 3rd computation \"raining + increased =  flood \" shows less signficant relationship. This could be due to the low occurence of the words in the dataset and also the increased complexity of the words.\n",
        "\n",
        "We suspect that these results has a lot to do with the small size of the dataset since the smaller range of words and context will inevitably mean that a lot of the words will hold some level of similarity irrespective of their contextual meanings. Nonetheless as shown below, the model can be quite useful.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXynq75V0cQ6",
        "outputId": "d4ae18b4-95d7-48b2-8d87-83bb42ca7e48"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy import linalg, mat, dot\n",
        "\n",
        "#Arithmetic Computations\n",
        "\n",
        "#1st set\n",
        "# temperatures + increase =  warmer \n",
        "temp_increase = mat(wv_model.wv['temperatures'] + wv_model.wv['increase'])\n",
        "warmer = mat(wv_model.wv['warmer'])\n",
        "\n",
        "quality_1 = dot(temp_increase,warmer.T)/linalg.norm(temp_increase)/linalg.norm(warmer)\n",
        "print(quality_1)\n",
        "\n",
        "\n",
        "#2st set\n",
        "# carbon + oxygen =  co\n",
        "carbon_oxygen = mat(wv_model.wv['carbon'] + wv_model.wv['oxygen'])\n",
        "co = mat(wv_model.wv['co'])\n",
        "\n",
        "quality_2 = dot(carbon_oxygen,co.T)/linalg.norm(carbon_oxygen)/linalg.norm(co)\n",
        "print(quality_2)\n",
        "\n",
        "#3rd set\n",
        "# raining + increased =  flood \n",
        "raining_increased = mat(wv_model.wv['raining'] + wv_model.wv['increased'])\n",
        "flood = mat(wv_model.wv['flood'])\n",
        "\n",
        "quality_3 = dot(raining_increased,flood.T)/linalg.norm(raining_increased)/linalg.norm(flood)\n",
        "print(quality_3)\n",
        "\n",
        "#4rd set\n",
        "# burning + fossil + fuels =  carbon + dioxide \n",
        "b_fossil_fuel = mat(wv_model.wv['burning'] + wv_model.wv['fossil']  + wv_model.wv['fuels'])\n",
        "carbon_doxide = mat(wv_model.wv['carbon'] + wv_model.wv['dioxide'])\n",
        "\n",
        "quality_4 = dot(b_fossil_fuel,carbon_doxide.T)/linalg.norm(b_fossil_fuel)/linalg.norm(carbon_doxide)\n",
        "print(quality_4)\n",
        "\n",
        "#5th set\n",
        "# ice + melts =  sea + rises \n",
        "ice_melts = mat(wv_model.wv['ice'] + wv_model.wv['melts'])\n",
        "sea_rises = mat(wv_model.wv['sea'] + wv_model.wv['rises'])\n",
        "\n",
        "quality_5 = dot(ice_melts,sea_rises.T)/linalg.norm(ice_melts)/linalg.norm(sea_rises)\n",
        "print(quality_5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3664157]]\n",
            "[[0.56710255]]\n",
            "[[0.25954226]]\n",
            "[[0.51425445]]\n",
            "[[0.6261726]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aQeVnMVMWaB"
      },
      "source": [
        "**CM 1 - Part 4 -- Loading two pretrained models - GloVec vs Word2Vec**\n",
        "\n",
        "GloVec model: glove-wiki-gigaword-300\n",
        "\n",
        "Word2Vec model: word2vec-wiki-gigaword-300\n",
        "\n",
        "GloVec.               Word2Vec\n",
        "\n",
        "[[0.56682545]] -> [[0.53775185]]\n",
        "\n",
        "[[0.13740683]] -> [[0.10254703]]\n",
        "\n",
        "[[0.20376654]] -> [[0.21976838]]\n",
        "\n",
        "[[0.5637933]] -> [[0.46399045]]\n",
        "\n",
        "[[0.34571153]] -> [[0.3586582]]\n",
        "\n",
        "\n",
        "On the left side, we have the GloVec model results and the Word2Vec model results on the right.\n",
        "\n",
        "The 1st computation has higher score in pre-trained models compared to our trained model. This actaully makes sense because \"temperatures + increase =  warmer\" has clear sematic relationship. The pretrained models were able to deduce the relationship because of the vast training dataset. \n",
        "\n",
        "The 2nd computation has lower score in pre-trained models compared to our trained model. \"carbon + oxygen =  co\" words occur relatively frequently in our very limited dataset and the simplicity of the word \"co\" allows our trained model to deduce a relationship. However, in real world or in larger diverse datasets \"carbon + oxygen =  co\" may not have significant and accurate relationship; the words might not occur close to each other frequently and their infrequent occureneces might be negligible in vast datasets. Therefore, pretrained models deduce this relationship to be insignificant. \n",
        "\n",
        "The 4th computation has good score in pre-trained models compared to our trained model. This actaully makes sense because \"burning + fossil + fuels =  carbon + dioxide\" words in a single sentences are widely available in wikipidea and google news. The pretrained models were able to deduce the relationship because of the vast training dataset. Similarly, our trained model also deduced that this relationship is significant because these words freuqeunly appeared in sentences in our training data. \n",
        "\n",
        "As was expected the arithmetic computation makes more sense in pre-trained model compared to the model that we trained on the **climate_fever** corpus. This is mainly because the climate fever dataset is small, hence a lot of context has been lost.\n",
        "\n",
        "An interesting observation of the above comparison is the similarity between the GloVec similarites and WordVec. They even share same directions for the 3rd set of words and we think that can be attributed to how generic both models are. This is mainly because of vast and diverse dataset they are trained on. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n3WowY-2Yd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db58712-43e5-429a-f4a4-1c4d69a203b2"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "info = api.info()\n",
        "model_gv_load = api.load(\"glove-wiki-gigaword-50\") \n",
        "\n",
        "#1st set\n",
        "# temperatures + increase =  warmer \n",
        "temp_increase = mat(model_gv_load['temperatures'] + model_gv_load['increase'])\n",
        "warmer = mat(model_gv_load['warmer'])\n",
        "\n",
        "quality_gv_1 = dot(temp_increase,warmer.T)/linalg.norm(temp_increase)/linalg.norm(warmer)\n",
        "print(quality_gv_1)\n",
        "\n",
        "#2st set\n",
        "# carbon + oxygen =  co\n",
        "carbon_oxygen = mat(model_gv_load['carbon'] + model_gv_load['oxygen'])\n",
        "co = mat(model_gv_load['co'])\n",
        "\n",
        "quality_gv_2 = dot(carbon_oxygen,co.T)/linalg.norm(carbon_oxygen)/linalg.norm(co)\n",
        "print(quality_gv_2)\n",
        "\n",
        "#3rd set\n",
        "# raining + increased =  flood \n",
        "raining_increased = mat(model_gv_load['raining'] + model_gv_load['increased'])\n",
        "flood = mat(model_gv_load['flood'])\n",
        "\n",
        "quality_gv_3 = dot(raining_increased,flood.T)/linalg.norm(raining_increased)/linalg.norm(flood)\n",
        "print(quality_gv_3)\n",
        "\n",
        "#4rd set\n",
        "# burning + fossil + fuels =  carbon + dioxide \n",
        "b_fossil_fuel = mat(model_gv_load['burning'] + model_gv_load['fossil']  + model_gv_load['fuels'])\n",
        "carbon_doxide = mat(model_gv_load['carbon'] + model_gv_load['dioxide'])\n",
        "\n",
        "quality_gv_4 = dot(b_fossil_fuel,carbon_doxide.T)/linalg.norm(b_fossil_fuel)/linalg.norm(carbon_doxide)\n",
        "print(quality_gv_4)\n",
        "\n",
        "#5th set\n",
        "# ice + melts =  sea + rises \n",
        "ice_melts = mat(model_gv_load['ice'] + model_gv_load['melts'])\n",
        "sea_rises = mat(model_gv_load['sea'] + model_gv_load['rises'])\n",
        "\n",
        "quality_gv_5 = dot(ice_melts,sea_rises.T)/linalg.norm(ice_melts)/linalg.norm(sea_rises)\n",
        "print(quality_gv_5)\n",
        "\n",
        "#6th set\n",
        "# King - man + woman = queen\n",
        "king_man_woman = mat(model_gv_load['king'] + model_gv_load['woman'] + model_gv_load['man'])\n",
        "queen = mat(model_gv_load['queen'])\n",
        "\n",
        "quality_gv_6 = dot(king_man_woman,queen.T)/linalg.norm(king_man_woman)/linalg.norm(queen)\n",
        "print(quality_gv_6)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.56682545]]\n",
            "[[0.13740683]]\n",
            "[[0.20376654]]\n",
            "[[0.5637933]]\n",
            "[[0.34571153]]\n",
            "[[0.5455089]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb_9hP0pPG1x",
        "outputId": "6b9d811a-0335-4d00-b169-1403bcd346a1"
      },
      "source": [
        "info = api.info() \n",
        "model_wv_loaded = api.load(\"word2vec-google-news-300\") \n",
        "\n",
        "# Training pretrained models on corpus dataset\n",
        "# model_wv_loaded.train(corpus)\n",
        "\n",
        "#1st set\n",
        "# temperatures + increase =  warmer \n",
        "temp_increase = mat(model_wv_loaded['temperatures'] + model_wv_loaded['increase'])\n",
        "warmer = mat(model_wv_loaded.wv['warmer'])\n",
        "\n",
        "quality_w2v_1 = dot(temp_increase,warmer.T)/linalg.norm(temp_increase)/linalg.norm(warmer)\n",
        "print(quality_w2v_1)\n",
        "\n",
        "\n",
        "#2st set\n",
        "# carbon + oxygen =  co\n",
        "carbon_oxygen = mat(model_wv_loaded['carbon'] + model_wv_loaded['oxygen'])\n",
        "co = mat(model_wv_loaded.wv['co'])\n",
        "\n",
        "quality_w2v_2 = dot(carbon_oxygen,co.T)/linalg.norm(carbon_oxygen)/linalg.norm(co)\n",
        "print(quality_w2v_2)\n",
        "\n",
        "#3rd set\n",
        "# raining + increased =  flood \n",
        "raining_increased = mat(model_wv_loaded['raining'] + model_wv_loaded['increased'])\n",
        "flood = mat(model_wv_loaded.wv['flood'])\n",
        "\n",
        "quality_w2v_3 = dot(raining_increased,flood.T)/linalg.norm(raining_increased)/linalg.norm(flood)\n",
        "print(quality_w2v_3)\n",
        "\n",
        "#4rd set\n",
        "# burning + fossil + fuels =  carbon + dioxide \n",
        "b_fossil_fuel = mat(model_wv_loaded['burning'] + model_wv_loaded['fossil']  + model_wv_loaded['fuels'])\n",
        "carbon_doxide = mat(model_wv_loaded['carbon'] + model_wv_loaded['dioxide'])\n",
        "\n",
        "quality_w2v_4 = dot(b_fossil_fuel,carbon_doxide.T)/linalg.norm(b_fossil_fuel)/linalg.norm(carbon_doxide)\n",
        "print(quality_w2v_4)\n",
        "\n",
        "#5th set\n",
        "# ice + melts =  sea + rises \n",
        "ice_melts = mat(model_wv_loaded['ice'] + model_wv_loaded['melts'])\n",
        "sea_rises = mat(model_wv_loaded['sea'] + model_wv_loaded['rises'])\n",
        "\n",
        "quality_w2v_5 = dot(ice_melts,sea_rises.T)/linalg.norm(ice_melts)/linalg.norm(sea_rises)\n",
        "print(quality_w2v_5)\n",
        "\n",
        "#6th set\n",
        "# King - man + woman = queen\n",
        "king_man_woman = mat(model_wv_loaded['king'] + model_wv_loaded['woman'] + model_wv_loaded['man'])\n",
        "queen = mat(model_wv_loaded['queen'])\n",
        "\n",
        "quality_gv_6 = dot(king_man_woman,queen.T)/linalg.norm(king_man_woman)/linalg.norm(queen)\n",
        "print(quality_gv_6)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.53775185]]\n",
            "[[0.10254703]]\n",
            "[[0.21976836]]\n",
            "[[0.46399045]]\n",
            "[[0.3586582]]\n",
            "[[0.5243245]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMryjlaWL0rV"
      },
      "source": [
        "Note: We encountered some issues training the pre-trained model on the climate dataset but we were not bothered about this since the climate\n",
        "dataset is very small compared to that of the pre-trained model so changes to similarity between words if any would be minimal."
      ]
    }
  ]
}